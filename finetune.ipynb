{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"16RkhWGjrwgen20M4cVb-evwVXs4GZ9i3","authorship_tag":"ABX9TyNSNMLgMHjEJ6E3uG+rvzMe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Fine\n"],"metadata":{"id":"f93PvTXg_N31"}},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"jgdKE5WTQrHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwJH6lsGQI-8"},"outputs":[],"source":["from datasets import Dataset\n","import pandas as pd\n","import re\n","\n","# Load the CSV dataset\n","dataset = Dataset.from_pandas(pd.read_csv('/content/drive/MyDrive/Project_1/counsel_chat.csv'), encoding='latin1')\n","\n","# Preprocess the dataset\n","def preprocess_example(example):\n","    # Remove non-text symbols using regex\n","    example['user'] = re.sub(r'[^\\w\\s\\'\",!?]', '', example['user'])\n","    example['therapist'] = re.sub(r'[^\\w\\s\\'\",!?]', '', example['therapist'])\n","\n","    return example\n","\n","dataset = dataset.map(preprocess_example)"]},{"cell_type":"code","source":["from datasets import train_test_split\n","\n","train_dataset, val_dataset = train_test_split(dataset, test_size=0.2)"],"metadata":{"id":"duIr4qrdRnIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration, Trainer, TrainingArguments\n","\n","# Load the tokenizer and model\n","tokenizer = BlenderbotTokenizer.from_pretrained('facebook/blenderbot-400M-distill')\n","model = BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')\n","\n","# Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10\n",")\n","\n","# Define the data collator\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","# Define the trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collator\n",")\n","\n","# Fine-tune the model\n","trainer.train()\n"],"metadata":{"id":"C3HyAq0QRpVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model('fine-tuned-model')"],"metadata":{"id":"Tvy10wmCRrZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BlenderbotForConditionalGeneration\n","\n","# Load the fine-tuned model\n","model = BlenderbotForConditionalGeneration.from_pretrained('fine-tuned-model')\n","\n","# Generate responses for some example inputs\n","inputs = [\"user: Hello\", \"user: How are you?\"]\n","for input in inputs:\n","    input_ids = tokenizer.encode(input, return_tensors='pt')\n","    output_ids = model.generate(input_ids)\n","    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    print(\"Bot:\", response)"],"metadata":{"id":"1anKp78CRthB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install --upgrade torch transformers"],"metadata":{"id":"pb0cnavkF9J-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install transformers[torch]"],"metadata":{"id":"PBSSkBPKJz_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install accelerate -U"],"metadata":{"id":"w94aJcU5LGck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import BlenderbotTokenizer, BlenderbotForCausalLM, AdamW\n","\n","# Load your dataset into a pandas DataFrame\n","df = pd.read_csv(\"/content/drive/MyDrive/Project_1/counsel_chat.csv\", encoding='ISO-8859-1')\n","\n","# Prepare the conversations from your dataset\n","conversations = []\n","for _, row in df.iterrows():\n","    conversation = [\n","        row[\"user\"],\n","        row[\"therapist\"]\n","    ]\n","    conversations.append(conversation)\n","\n","# Load the tokenizer and model\n","tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n","model = BlenderbotForCausalLM.from_pretrained(\"facebook/blenderbot-400M-distill\")\n","\n","# Move the model to GPU\n","model.to(\"cuda\")\n","\n","# Tokenize the conversations\n","tokenized_conversations = tokenizer(conversations, padding=True, truncation=True, return_tensors=\"pt\", max_length=512, add_special_tokens=True)\n","\n","# Convert the tokenized conversations into a torch dataset\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __getitem__(self, idx):\n","        return {key: val[idx].to(\"cuda\") for key, val in self.encodings.items()}\n","\n","    def __len__(self):\n","        return len(self.encodings.input_ids)\n","\n","dataset = MyDataset(tokenized_conversations)\n","\n","# Define the training arguments\n","num_train_epochs = 3\n","per_device_train_batch_size = 1\n","learning_rate = 1e-4\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_train_epochs):\n","    for i in range(0, len(dataset), per_device_train_batch_size):\n","        batch = {key: val[i:i+per_device_train_batch_size].to(\"cuda\") for key, val in tokenized_conversations.items()}\n","\n","        # Ensure all sequences in the batch have the same length\n","        max_len = max(len(seq) for seq in batch[\"input_ids\"])\n","        for key in batch:\n","            batch[key] = torch.stack([torch.cat([seq, torch.tensor([tokenizer.pad_token_id] * (max_len - len(seq)))]).to(\"cuda\") for seq in batch[key]])[0]\n","\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","# Save the fine-tuned model\n","model.save_pretrained(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")\n"],"metadata":{"id":"-VLjH9SC_U7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install transformers[torch,accelerate]"],"metadata":{"id":"4O6XV8QHYg6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_sequence_length = 512  # Set your desired maximum sequence length\n","df['concatenated'] = df['concatenated'].apply(lambda x: x[:max_sequence_length])"],"metadata":{"id":"NmC-HYjpdDii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, tokenizer, text_column):\n","        self.examples = tokenizer(\n","            text_column.tolist(),\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","\n","    def __len__(self):\n","        return len(self.examples[\"input_ids\"])\n","\n","    def __getitem__(self, idx):\n","        return {\"input_ids\": self.examples[\"input_ids\"][idx]}\n","\n","# Create the custom dataset\n","train_dataset = MyDataset(tokenizer, df['concatenated'])\n"],"metadata":{"id":"QOOBbirCdGft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Updated sequence lengths:\", [len(tokenizer.encode(text)) for text in df['concatenated']])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhbUK_RndJ4v","executionInfo":{"status":"ok","timestamp":1701594973244,"user_tz":-330,"elapsed":1117,"user":{"displayName":"C1_01_Aman_Jain","userId":"13267410074613304951"}},"outputId":"a49a9d9c-a930-47a3-84e1-c69cd788c422"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated sequence lengths: [108, 123, 124, 111, 116, 109, 105, 107, 116, 116, 111, 104, 117, 109, 116, 112, 119, 118, 127, 105, 98, 116, 117, 113, 132, 116, 127, 142, 114, 119, 112, 133, 121, 119, 121, 137, 107, 121, 120, 122, 119, 133, 120, 96, 109, 119, 112, 109, 137, 102, 121, 105, 111, 111, 115, 116, 132, 119, 123, 106, 117, 127, 115, 121, 140, 114, 125, 123, 126, 109, 112, 115, 121, 108, 104, 112, 116, 115, 121, 111, 113, 123, 68, 117, 119, 122, 109, 108, 125, 146, 120, 88, 132, 141, 118, 111, 122, 118, 123, 115, 106, 142, 142, 114, 101, 150, 142, 109, 116, 108, 112, 132, 113, 103, 124, 115, 45, 114, 131, 115, 111, 111, 85, 142, 112, 114, 119, 110, 118, 122, 118, 111, 112, 124, 104, 119, 123, 109, 112, 99, 129, 114, 124, 117, 131, 118, 120, 130, 123, 127, 114, 108, 127, 109, 101, 121, 110, 112, 122, 92, 110, 115, 100, 128, 118, 119, 106, 134, 131, 128, 129, 126, 109, 112, 110, 116, 121, 117, 122, 125, 102, 122, 118, 111, 120, 118, 112, 121, 111, 120, 111, 118, 115, 120, 116, 107, 113, 121, 119, 112, 110, 138, 114, 112, 129, 112, 121, 109, 123, 122, 105, 114, 100, 109, 101, 126, 110, 126, 101, 130, 130, 117, 116, 121, 130, 110, 127, 118, 139, 107, 114, 109, 126, 110, 112, 119, 120, 62, 110, 124, 119, 135, 112, 84, 113, 102, 113, 117, 113, 107, 108, 104, 97, 114, 128, 125, 113, 107, 124, 116, 121, 120, 122, 104, 116, 106, 117, 114, 125, 114, 111, 109, 126, 104, 114, 114, 116, 106, 116, 123, 134, 123, 126, 115, 112, 114, 111, 117, 119, 134, 117, 110, 125, 153, 123, 111, 133, 122, 133, 109, 129, 122, 120, 120, 104, 118, 123, 117, 113, 117, 118, 113, 129, 118, 112, 129, 112, 98, 116, 111, 119, 114, 104, 94, 102, 108, 100, 119, 116, 114, 116, 108, 97, 118, 100, 108, 124, 109, 127, 120, 115, 115, 108, 120, 113, 120, 111, 115, 123, 120, 108, 123, 109, 126, 124, 125, 107, 124, 125, 125, 117, 121, 119, 125, 125, 125, 128, 112, 109, 121, 115, 112, 143, 110, 101, 107, 116, 115, 122, 114, 116, 137, 114, 127, 122, 127, 116, 113, 141, 73, 83, 123, 105, 121, 114, 141, 116, 117, 125, 115, 110, 117, 118, 125, 106, 114, 105, 129, 122, 118, 111, 118, 106, 109, 112, 110, 118, 120, 117, 120, 118, 111, 117, 46, 117, 115, 112, 141, 145, 147, 120, 112, 109, 116, 125, 117, 113, 112, 121, 119, 116, 110, 119, 118, 109, 113, 105, 110, 106, 121, 126, 122, 102, 114, 113, 103, 112, 127, 121, 117, 133, 128, 121, 112, 129, 121, 123, 121, 131, 111, 119, 109, 120, 130, 129, 163, 112, 118, 116, 119, 118, 105, 111, 118, 123, 128, 154, 119, 128, 126, 116, 116, 111, 108, 119, 151, 104, 120, 117, 79, 110, 125, 114, 90, 94, 130, 116, 108, 122, 111, 112, 114, 105, 121, 123, 111, 110, 102, 123, 112, 109, 128, 109, 130, 123, 120, 144, 139, 127, 113, 102, 102, 157, 108, 127, 106, 112, 110, 115, 135, 106, 103, 130, 112, 110, 119, 122, 116, 106, 100, 113, 103, 109, 140, 114, 116, 110, 121, 121, 112, 112, 98, 144, 112, 111, 99, 106, 112, 106, 109, 112, 110, 122, 126, 104, 77, 111, 118, 93, 108, 109, 111, 101, 99, 158, 115, 127, 127, 102, 132, 109, 88, 115, 114, 110, 128, 122, 114, 115, 121, 117, 105, 111, 159, 111, 105, 111, 109, 119, 116, 128, 108, 120, 124, 107, 122, 128, 110, 127, 99, 116, 118, 117, 114, 150, 138, 111, 113, 116, 116, 132, 123, 113, 108, 126, 115, 100, 112, 111, 109, 119, 122, 107, 124, 131, 115, 119, 122, 125, 121, 119, 117, 111, 121, 120, 111, 120, 102, 131, 150, 134, 124, 113, 112, 124, 110, 96, 118, 144, 121, 133, 104, 98, 94, 115, 112, 104, 92, 111, 150, 143, 127, 107, 116, 122, 124, 125, 119, 109, 107, 96, 114, 109, 131, 118, 132, 143, 119, 119, 123, 104, 103, 139, 111, 110, 117, 108, 78, 104, 87, 130, 106, 118, 114, 112, 72, 114, 100, 119, 108, 117, 115, 107, 117, 111, 117, 114, 113, 109, 118, 107, 108, 148, 132, 150, 121, 124, 112, 110, 116, 102, 141, 119, 118, 117, 110, 119, 149, 105, 116, 111, 115, 110, 153, 118, 76, 89, 110, 115, 109, 112, 123, 103, 106, 140, 109, 118, 115, 117, 102, 106, 103, 111, 120, 106, 114, 149, 118, 120, 110, 123, 145, 111, 99, 104, 109, 107, 129, 107, 103, 102, 124, 123, 113, 132, 114, 117, 110, 110, 119, 121, 130, 112, 133, 105, 117, 113, 151, 119, 115, 85, 76, 105, 97, 119, 118, 116, 106, 117, 113, 110, 100, 111, 112, 128, 127, 109, 115, 118, 121, 111, 115, 125, 113, 107, 125, 106, 117, 114, 121, 123, 114, 119, 105, 112, 102, 103, 112, 135, 109, 110, 126, 115, 131, 106, 118, 115, 117, 102, 109, 113, 123, 124, 118, 112, 136, 120, 153, 93, 116, 119, 112, 100, 98, 114, 110, 135, 113, 119, 115, 111, 123, 121, 112, 91, 109, 111, 109, 112, 118, 117, 113, 122, 115, 116, 110, 115, 152, 115, 101, 106, 115, 106, 114, 99, 109, 113, 105, 114, 116, 117, 93, 106, 111, 118, 120, 141, 118, 130, 109, 118, 116, 108, 127, 114, 130, 128, 136, 117, 118, 115, 120, 116, 140, 112, 122, 109, 131, 129, 112, 110, 114, 113, 106, 117, 113, 42, 118, 119, 145, 133, 128, 121, 112, 67, 115, 121, 126, 110, 129, 140, 121, 105, 119, 117, 114, 117, 130, 102, 117, 118, 106, 107, 126, 111, 114, 135, 102, 103, 107, 120, 114, 121, 130, 115, 111, 61, 107, 110, 121, 116, 138, 92, 116, 110, 132, 111, 110, 108, 102, 130, 109, 120, 107, 105, 116, 114, 105, 112, 126, 116, 111, 118, 318, 118, 106, 117, 126, 115, 115, 116, 110, 104, 87, 117, 111, 118, 117, 116, 126, 116, 107, 109, 105, 117, 116, 127, 121, 108, 116, 119, 124, 110, 125, 123, 118, 195, 116, 109, 103, 118, 139, 107, 119, 122, 135, 116, 122, 123, 121, 114, 126, 84, 108, 110, 96, 120, 104, 107, 106, 122, 110, 116, 112, 114, 119, 121, 52, 109, 114, 108, 121, 107, 106, 69, 84, 99, 137, 106, 120, 102, 103, 107, 110, 96, 119, 103, 106, 118, 97, 118, 145, 127, 127, 124, 133, 104, 119, 118, 43, 112, 118, 88, 125, 111, 135, 137, 112, 154, 118, 111, 104, 122, 108, 110, 124, 105, 96, 113, 114, 122, 106, 131, 123, 107, 114, 120, 110, 110, 101, 117, 111, 110, 111, 125, 111, 112, 118, 126, 113, 118, 123, 112, 99, 117, 113, 112, 71, 122, 41, 112, 134, 107, 116, 120, 119, 145, 103, 112, 115, 117, 108, 129, 135, 75, 116, 114, 137, 110, 108, 134, 118, 111, 137, 121, 109, 112, 118, 122, 115, 106, 103, 116, 108, 120, 118, 106, 117, 120, 109, 126, 108, 124, 120, 109, 109, 34, 116, 112, 116, 109, 111, 121, 116, 125, 106, 111, 108, 125, 85, 82, 113, 125, 136, 99, 107, 134, 121, 107, 119, 101, 97, 102, 34, 101, 109, 104, 127, 100, 99, 123, 112, 109, 124, 97, 108, 107, 118, 113, 61, 104, 104, 102, 124, 127, 111, 116, 116, 80, 116, 121, 110, 85, 105, 118, 139, 113, 160, 119, 139, 109, 118, 116, 88, 106, 98, 91, 127, 102, 118, 102, 113, 71, 108, 105, 128, 112, 118, 106, 119, 86, 121, 130, 97, 115, 117, 123, 125, 101, 115, 112, 113, 108, 128, 104, 125, 109, 107, 127, 122, 108, 103, 125, 113, 124, 124, 126, 98, 91, 112, 123, 109, 126, 85, 101, 121, 118, 125, 139, 114, 112, 114, 112, 122, 129, 143, 121, 127, 115, 115, 122, 100, 123, 111, 108, 119, 107, 113, 116, 93, 127, 126, 125, 119, 120, 130, 113, 112, 124, 122, 124, 113, 105, 121, 127, 117, 106, 122, 123, 111, 111, 108, 121, 71]\n"]}]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AdamW\n","import pandas as pd\n","\n","# Load and preprocess your dataset\n","# Specify a different encoding if needed\n","dataset_path = '/content/drive/MyDrive/Project_1/processed_counsel_chat.csv'\n","df = pd.read_csv(dataset_path, encoding='ISO-8859-1')\n","df['concatenated'] = df['user'] + df['therapist']\n","df.to_csv('/content/drive/MyDrive/Project_1/processed_counsel_chat.csv', index=False)\n","\n","# Tokenize the dataset\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","# Define a custom dataset class\n","class MyDataset(Dataset):\n","    def __init__(self, tokenizer, text_column):\n","        self.examples = tokenizer(\n","            text_column.tolist(),\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","\n","    def __len__(self):\n","        return len(self.examples[\"input_ids\"])\n","\n","    def __getitem__(self, idx):\n","        return {\"input_ids\": self.examples[\"input_ids\"][idx]}\n","\n","# Create the custom dataset\n","train_dataset = MyDataset(tokenizer, df['concatenated'])\n","\n","# Create DataLoader\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","\n","# Load the pre-trained GPT-2 model\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Define optimizer\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Training loop\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_dataloader:\n","        optimizer.zero_grad()\n","        inputs = batch[\"input_ids\"]\n","        outputs = model(inputs, labels=inputs)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","# Save the fine-tuned model\n","model.save_pretrained(\"./your_fine_tuned_model\")\n"],"metadata":{"id":"UGurKNOJdbLJ"},"execution_count":null,"outputs":[]}]}